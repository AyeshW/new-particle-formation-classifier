{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src.utils import load_json, save_json\n",
    "from src.metrics import multiclass_and_binary_metrics\n",
    "\n",
    "mlflow.set_experiment(\"iml2025_project\")\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "os.makedirs(\"../logs/metrics\", exist_ok=True)\n",
    "\n",
    "# load train/test and class_list\n",
    "train = pd.read_csv(\"../data/train_fe.csv\")\n",
    "train[\"class2\"] = (train[\"class4\"] != \"nonevent\").astype(int)\n",
    "X_train = train.drop(columns=[\"class4\",\"class2\"])\n",
    "y_class4 = train[\"class4\"].values\n",
    "y_binary = train[\"class2\"].values\n",
    "\n",
    "test = pd.read_csv(\"../data/test_fe.csv\")\n",
    "X_test = test.drop(columns=[\"id\"])\n",
    "\n",
    "class_list = load_json(\"../models/class_list.json\")\n",
    "label_to_idx = {lab:i for i,lab in enumerate(class_list)}\n",
    "nonevent_idx = label_to_idx[\"nonevent\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc8307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Load ensemble weights and ensemble OOF multiclass raw\n",
    "ens_meta = load_json(\"../models/ensemble_weights.json\")\n",
    "model_names = ens_meta[\"model_names\"]\n",
    "weights = np.array(ens_meta[\"weights\"])\n",
    "print(\"Ensemble models & weights:\", list(zip(model_names, weights)))\n",
    "\n",
    "ens_oof_raw = np.load(\"../models/oof_ensemble_multiclass_raw.npy\")  # (n_train, k)\n",
    "# derive raw p_event OOF\n",
    "oof_p_event_raw = 1.0 - ens_oof_raw[:, nonevent_idx]\n",
    "np.save(\"../models/oof_ensemble_p_event_raw.npy\", oof_p_event_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc0a256",
   "metadata": {},
   "source": [
    "**Calibration Curve - Before Calibration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c747052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_binary, oof_p_event_raw, n_bins=10)\n",
    "\n",
    "plt.plot(prob_pred, prob_true, marker='o', label = 'Ensemble Classifier')\n",
    "plt.plot([0, 1], [0, 1], '--', label = 'Ideal Calibration')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Plot')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.savefig('../logs/eda_plots/calibration_plot_ensemble_not_calibrated.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca17bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Fit Platt calibrator on OOF p_event (binary only)\n",
    "with mlflow.start_run(run_name=\"06_platt_calibration_binary\"):\n",
    "    calibrator = LogisticRegression(max_iter=2000)\n",
    "    calibrator.fit(oof_p_event_raw.reshape(-1,1), y_binary)\n",
    "    joblib.dump(calibrator, \"../models/calibrator_platt_p_event.joblib\")\n",
    "\n",
    "    oof_p_event_cal = calibrator.predict_proba(oof_p_event_raw.reshape(-1,1))[:,1]\n",
    "    bin_logloss_cal = log_loss(y_binary, oof_p_event_cal)\n",
    "    bin_acc_cal = accuracy_score(y_binary, (oof_p_event_cal > 0.5).astype(int))\n",
    "    mlflow.log_metric(\"bin_logloss_oof_calibrated\", float(bin_logloss_cal))\n",
    "    mlflow.log_metric(\"bin_accuracy_oof_calibrated\", float(bin_acc_cal))\n",
    "    save_json({\"bin_logloss_oof_calibrated\": float(bin_logloss_cal), \"bin_accuracy_oof_calibrated\": float(bin_acc_cal)}, \"../models/calibration_platt_metrics.json\")\n",
    "    print(\"OOF binary logloss after Platt:\", bin_logloss_cal, \"acc:\", bin_acc_cal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc79d9",
   "metadata": {},
   "source": [
    "**Calibration Curve - After Calibration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a70da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_binary, oof_p_event_cal, n_bins=10)\n",
    "\n",
    "plt.plot(prob_pred, prob_true, marker='o', label = 'Ensemble Classifier')\n",
    "plt.plot([0, 1], [0, 1], '--', label = 'Ideal Calibration')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Plot')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.savefig('../logs/eda_plots/calibration_plot_ensemble_calibrated.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9298015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# load trained full models (from Notebook 4)\n",
    "# model_names: [\"extratrees\",\"xgb\",\"rf\"] (ordered)\n",
    "models = []\n",
    "for name in model_names:\n",
    "    path = f\"../models/best_{('ex' if name=='extratrees' else name)}_full.joblib\"\n",
    "    # note: ExtraTrees used key \"best_extratrees_full.joblib\" in Notebook 4; others similar\n",
    "    # Try to handle possible naming differences robustly:\n",
    "    if name == \"extratrees\":\n",
    "        path = \"../models/best_extratrees_full.joblib\"\n",
    "    elif name == \"xgb\":\n",
    "        path = \"../models/best_xgb_full.joblib\"\n",
    "    elif name == \"rf\":\n",
    "        path = \"../models/best_rf_full.joblib\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Expected model file not found: {path}\")\n",
    "    models.append(joblib.load(path))\n",
    "\n",
    "print(\"Loaded models:\", [type(m).__name__ for m in models])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6822e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Predict test multiclass probs per model and ensemble\n",
    "probs_list_test = [m.predict_proba(X_test) for m in models]   # list of (n_test, k)\n",
    "# Weighted average (order must match model_names & weights)\n",
    "probs_test_ens_raw = sum(weights[i] * probs_list_test[i] for i in range(len(probs_list_test)))\n",
    "# normalize rows (numerical safety)\n",
    "probs_test_ens_raw = probs_test_ens_raw / probs_test_ens_raw.sum(axis=1, keepdims=True)\n",
    "np.save(\"../models/probs_test_ensemble_raw.npy\", probs_test_ens_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085419f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Derive p_event_test_raw and apply Platt calibrator (binary only)\n",
    "p_nonevent_test_raw = probs_test_ens_raw[:, nonevent_idx]\n",
    "p_event_test_raw = 1.0 - p_nonevent_test_raw\n",
    "\n",
    "calibrator = joblib.load(\"../models/calibrator_platt_p_event.joblib\")\n",
    "p_event_test_cal = calibrator.predict_proba(p_event_test_raw.reshape(-1,1))[:,1]\n",
    "np.save(\"../models/p_event_test_calibrated.npy\", p_event_test_cal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c380e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Determine class4 submission label from raw ensemble multiclass (argmax) and build submission\n",
    "# class4 by argmax on raw ensemble multiclass probabilities (not modified by binary calibrator)\n",
    "class4_idx = np.argmax(probs_test_ens_raw, axis=1)\n",
    "# convert indices to string labels using the original class_list order and LabelEncoder mapping:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(class_list)   # must be same order as earlier\n",
    "class4_labels = le.inverse_transform(class4_idx)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"class4\": class4_labels,\n",
    "    \"p\": p_event_test_cal\n",
    "})\n",
    "submission_path = \"../data/submission_calibrated.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "mlflow.log_artifact(submission_path)\n",
    "print(\"Saved submission to:\", submission_path)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6bf981",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"class4\": class4_labels,\n",
    "    \"p\": p_event_test_raw\n",
    "})\n",
    "submission_path = \"../data/submission_not_calibrated.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "mlflow.log_artifact(submission_path)\n",
    "print(\"Saved submission to:\", submission_path)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src.metrics import get_aggregated_score\n",
    "\n",
    "et_aggregated_score = get_aggregated_score(0.866667, 0.660000, 1.402295)\n",
    "xgb_aggregated_score = get_aggregated_score(0.866667, 0.684444, 1.376222)\n",
    "rf_aggregated_score = get_aggregated_score(0.860000, 0.633333, 1.410922)\n",
    "ensemble_aggregated_score = get_aggregated_score(0.8711111111111111, 0.6777777777777778, 1.3759060730812251)\n",
    "\n",
    "perplexity_after_calibration = float(np.exp(0.32977921968787743))\n",
    "print(\"Perplexity after calibration:\", perplexity_after_calibration)\n",
    "calibrated_aggregated_score = get_aggregated_score(0.8688888888888889, 0.6777777777777778, perplexity_after_calibration)\n",
    "print(\"Aggregated Scores:\")\n",
    "print(f\"ExtraTrees: {et_aggregated_score:.6f}\")\n",
    "print(f\"XGBoost: {xgb_aggregated_score:.6f}\")\n",
    "print(f\"RandomForest: {rf_aggregated_score:.6f}\")\n",
    "print(f\"Ensemble (not calibrated): {ensemble_aggregated_score:.6f}\")\n",
    "print(f\"Ensemble (calibrated): {calibrated_aggregated_score:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npf-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
